-- SELECT ! true;
-- SELECT ! false;
-- SELECT ! NULL;
-- SELECT 1 != 2;
-- SELECT 1 != '2';
-- SELECT true != NULL;
-- SELECT NULL != NULL;
-- SELECT 2 % 1.8;
-- SELECT MOD(2, 1.8);
-- SELECT 3 & 5;
-- SELECT 2 * 3;
-- SELECT 1 + 2;
-- SELECT 2 - 1;
-- SELECT 3 / 2;
-- SELECT 2L / 2L;
-- SELECT 1 < 2;
-- SELECT 1.1 < '1';
-- SELECT to_date('2009-07-30 04:17:52') < to_date('2009-07-30 04:17:52');
-- SELECT to_date('2009-07-30 04:17:52') < to_date('2009-08-01 04:17:52');
-- SELECT 1 < NULL;
-- SELECT 2 <= 2;
-- SELECT 1.0 <= '1';
-- SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-07-30 04:17:52');
-- SELECT to_date('2009-07-30 04:17:52') <= to_date('2009-08-01 04:17:52');
-- SELECT 1 <= NULL;
-- SELECT 2 <=> 2;
-- SELECT 1 <=> '1';
-- SELECT true <=> NULL;
-- SELECT NULL <=> NULL;
-- SELECT 1 != 2;
-- SELECT 1 != '2';
-- SELECT true != NULL;
-- SELECT NULL != NULL;
-- SELECT 2 = 2;
-- SELECT 1 = '1';
-- SELECT true = NULL;
-- SELECT NULL = NULL;
-- SELECT 2 == 2;
-- SELECT 1 == '1';
-- SELECT true == NULL;
-- SELECT NULL == NULL;
-- SELECT 2 > 1;
-- SELECT 2 > '1.1';
-- SELECT to_date('2009-07-30 04:17:52') > to_date('2009-07-30 04:17:52');
-- SELECT to_date('2009-07-30 04:17:52') > to_date('2009-08-01 04:17:52');
-- SELECT 1 > NULL;
-- SELECT 2 >= 1;
-- SELECT 2.0 >= '2.1';
-- SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-07-30 04:17:52');
-- SELECT to_date('2009-07-30 04:17:52') >= to_date('2009-08-01 04:17:52');
-- SELECT 1 >= NULL;
-- SELECT 3 ^ 5;
-- SELECT abs(-1);
-- SELECT acos(1);
-- SELECT acos(2);
-- SELECT acosh(1);
-- SELECT acosh(0);
-- SELECT add_months('2016-08-31', 1);
-- SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x);
-- SELECT aggregate(array(1, 2, 3), 0, (acc, x) -> acc + x, acc -> acc * 10);
-- SELECT true and true;
-- SELECT true and false;
-- SELECT true and NULL;
-- SELECT false and NULL;
-- SELECT any(col) FROM VALUES (true), (false), (false) AS tab(col);
-- SELECT any(col) FROM VALUES (NULL), (true), (false) AS tab(col);
-- SELECT any(col) FROM VALUES (false), (false), (NULL) AS tab(col);
-- SELECT approx_count_distinct(col1) FROM VALUES (1), (1), (2), (2), (3) tab(col1);
-- SELECT approx_percentile(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);
-- SELECT approx_percentile(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);
-- SELECT array(1, 2, 3);
-- SELECT array_contains(array(1, 2, 3), 2);
-- SELECT array_distinct(array(1, 2, 3, null, 3));
-- SELECT array_except(array(1, 2, 3), array(1, 3, 5));
-- SELECT array_intersect(array(1, 2, 3), array(1, 3, 5));
-- SELECT array_join(array('hello', 'world'), ' ');
-- SELECT array_join(array('hello', null ,'world'), ' ');
-- SELECT array_join(array('hello', null ,'world'), ' ', ',');
-- SELECT array_max(array(1, 20, null, 3));
-- SELECT array_min(array(1, 20, null, 3));
-- SELECT array_position(array(3, 2, 1), 1);
-- SELECT array_remove(array(1, 2, 3, null, 3), 3);
-- SELECT array_repeat('123', 2);
-- SELECT array_sort(array(5, 6, 1), (left, right) -> case when left < right then -1 when left > right then 1 else 0 end);
-- SELECT array_sort(array('bc', 'ab', 'dc'), (left, right) -> case when left is null and right is null then 0 when left is null then -1 when right is null then 1 when left < right then 1 when left > right then -1 else 0 end);
-- SELECT array_sort(array('b', 'd', null, 'c', 'a'));
-- SELECT array_union(array(1, 2, 3), array(1, 3, 5));
-- SELECT arrays_overlap(array(1, 2, 3), array(3, 4, 5));
-- SELECT arrays_zip(array(1, 2, 3), array(2, 3, 4));
-- SELECT arrays_zip(array(1, 2), array(2, 3), array(3, 4));
-- SELECT ascii('222');
-- SELECT ascii(2);
-- SELECT asin(0);
-- SELECT asin(2);
-- SELECT asinh(0);
-- SELECT assert_true(0 < 1);
-- SELECT atan(0);
-- SELECT atan2(0, 0);
-- SELECT atanh(0);
-- SELECT atanh(2);
-- SELECT avg(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT avg(col) FROM VALUES (1), (2), (NULL) AS tab(col);
-- SELECT base64('Spark SQL');
-- SELECT col1 FROM VALUES 1, 3, 5, 7 WHERE col1 BETWEEN 2 AND 5;
-- SELECT bin(13);
-- SELECT bin(-13);
-- SELECT bin(13.3);
-- SELECT bit_and(col) FROM VALUES (3), (5) AS tab(col);
-- SELECT bit_count(0);
-- SELECT bit_length('Spark SQL');
-- SELECT bit_or(col) FROM VALUES (3), (5) AS tab(col);
-- SELECT bit_xor(col) FROM VALUES (3), (5) AS tab(col);
-- SELECT bool_and(col) FROM VALUES (true), (true), (true) AS tab(col);
-- SELECT bool_and(col) FROM VALUES (NULL), (true), (true) AS tab(col);
-- SELECT bool_and(col) FROM VALUES (true), (false), (true) AS tab(col);
-- SELECT bool_or(col) FROM VALUES (true), (false), (false) AS tab(col);
-- SELECT bool_or(col) FROM VALUES (NULL), (true), (false) AS tab(col);
-- SELECT bool_or(col) FROM VALUES (false), (false), (NULL) AS tab(col);
-- SELECT bround(2.5, 0);
-- SELECT cardinality(array('b', 'd', 'c', 'a'));
-- SELECT cardinality(map('a', 1, 'b', 2));
-- SELECT cardinality(NULL);
-- SELECT CASE col1 WHEN 1 THEN 'one' WHEN 2 THEN 'two' ELSE '?' END FROM VALUES 1, 2, 3;
-- SELECT CASE col1 WHEN 1 THEN 'one' WHEN 2 THEN 'two' END FROM VALUES 1, 2, 3;
-- SELECT cast('10' as int);
-- SELECT cbrt(27.0);
-- SELECT ceil(-0.1);
-- SELECT ceil(5);
-- SELECT ceiling(-0.1);
-- SELECT ceiling(5);
-- SELECT char(65);
-- SELECT char_length('Spark SQL ');
-- SELECT CHAR_LENGTH('Spark SQL ');
-- SELECT CHARACTER_LENGTH('Spark SQL ');
-- SELECT character_length('Spark SQL ');
-- SELECT CHAR_LENGTH('Spark SQL ');
-- SELECT CHARACTER_LENGTH('Spark SQL ');
-- SELECT chr(65);
-- SELECT coalesce(NULL, 1, NULL);
-- SELECT collect_list(col) FROM VALUES (1), (2), (1) AS tab(col);
-- SELECT collect_set(col) FROM VALUES (1), (2), (1) AS tab(col);
-- SELECT concat('Spark', 'SQL');
-- SELECT concat(array(1, 2, 3), array(4, 5), array(6));
-- SELECT concat_ws(' ', 'Spark', 'SQL');
-- SELECT concat_ws('s');
-- SELECT conv('100', 2, 10);
-- SELECT conv(-10, 16, -10);
-- SELECT corr(c1, c2) FROM VALUES (3, 2), (3, 3), (6, 4) as tab(c1, c2);
-- SELECT cos(0);
-- SELECT cosh(0);
-- SELECT cot(1);
-- SELECT count(*) FROM VALUES (NULL), (5), (5), (20) AS tab(col);
-- SELECT count(col) FROM VALUES (NULL), (5), (5), (20) AS tab(col);
-- SELECT count(DISTINCT col) FROM VALUES (NULL), (5), (5), (10) AS tab(col);
-- SELECT count_if(col % 2 = 0) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);
-- SELECT count_if(col IS NULL) FROM VALUES (NULL), (0), (1), (2), (3) AS tab(col);
-- SELECT hex(count_min_sketch(col, 0.5d, 0.5d, 1)) FROM VALUES (1), (2), (1) AS tab(col);
-- SELECT covar_pop(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);
-- SELECT covar_samp(c1, c2) FROM VALUES (1,1), (2,2), (3,3) AS tab(c1, c2);
-- SELECT crc32('Spark');
-- SELECT name, age, count(*) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY cube(name, age);
-- SELECT a, b, cume_dist() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT current_catalog();
-- SELECT current_database();
-- SELECT current_date();
-- SELECT current_date;
-- SELECT current_timestamp();
-- SELECT current_timestamp;
-- SELECT current_timezone();
-- SELECT date_add('2016-07-30', 1);
-- SELECT date_format('2016-04-08', 'y');
-- SELECT date_from_unix_date(1);
-- eld - selects which part of the source should be extracted, and supported string values are as same as the fields of the equivalent function EXTRACT.
-- SELECT date_part('YEAR', TIMESTAMP '2019-08-12 01:00:00.123456');
-- SELECT date_part('week', timestamp'2019-08-12 01:00:00.123456');
-- SELECT date_part('doy', DATE'2019-08-12');
-- SELECT date_part('SECONDS', timestamp'2019-10-01 00:00:01.000001');
-- SELECT date_part('days', interval 1 year 10 months 5 days);
-- SELECT date_part('seconds', interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
-- SELECT date_sub('2016-07-30', 1);
-- SELECT date_trunc('YEAR', '2015-03-05T09:32:05.359');
-- SELECT date_trunc('MM', '2015-03-05T09:32:05.359');
-- SELECT date_trunc('DD', '2015-03-05T09:32:05.359');
-- SELECT date_trunc('HOUR', '2015-03-05T09:32:05.359');
-- SELECT date_trunc('MILLISECOND', '2015-03-05T09:32:05.123456');
-- SELECT datediff('2009-07-31', '2009-07-30');
-- SELECT datediff('2009-07-30', '2009-07-31');
-- SELECT day('2009-07-30');
-- SELECT dayofmonth('2009-07-30');
-- SELECT dayofweek('2009-07-30');
-- SELECT dayofyear('2016-04-09');
-- SELECT decode(encode('abc', 'utf-8'), 'utf-8');
-- SELECT degrees(3.141592653589793);
-- SELECT a, b, dense_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT 3 div 2;
-- SELECT e();
-- SELECT element_at(array(1, 2, 3), 2);
-- SELECT element_at(map(1, 'a', 2, 'b'), 2);
-- SELECT elt(1, 'scala', 'java');
-- SELECT encode('abc', 'utf-8');
-- SELECT every(col) FROM VALUES (true), (true), (true) AS tab(col);
-- SELECT every(col) FROM VALUES (NULL), (true), (true) AS tab(col);
-- SELECT every(col) FROM VALUES (true), (false), (true) AS tab(col);
-- SELECT exists(array(1, 2, 3), x -> x % 2 == 0);
-- SELECT exists(array(1, 2, 3), x -> x % 2 == 10);
-- SELECT exists(array(1, null, 3), x -> x % 2 == 0);
-- SELECT exists(array(0, null, 2, 3, null), x -> x IS NULL);
-- SELECT exists(array(1, 2, 3), x -> x IS NULL);
-- SELECT exp(0);
-- SELECT explode(array(10, 20));
-- SELECT explode_outer(array(10, 20));
-- SELECT expm1(0);
-- eld - selects which part of the source should be extracted
-- SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');
-- SELECT extract(week FROM timestamp'2019-08-12 01:00:00.123456');
-- SELECT extract(doy FROM DATE'2019-08-12');
-- SELECT extract(SECONDS FROM timestamp'2019-10-01 00:00:01.000001');
-- SELECT extract(days FROM interval 1 year 10 months 5 days);
-- SELECT extract(seconds FROM interval 5 hours 30 seconds 1 milliseconds 1 microseconds);
-- SELECT factorial(5);
-- SELECT filter(array(1, 2, 3), x -> x % 2 == 1);
-- SELECT filter(array(0, 2, 3), (x, i) -> x > i);
-- SELECT filter(array(0, null, 2, 3, null), x -> x IS NOT NULL);
-- SELECT find_in_set('ab','abc,b,ab,c,def');
-- SELECT first(col) FROM VALUES (10), (5), (20) AS tab(col);
-- SELECT first(col) FROM VALUES (NULL), (5), (20) AS tab(col);
-- SELECT first(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
-- SELECT first_value(col) FROM VALUES (10), (5), (20) AS tab(col);
-- SELECT first_value(col) FROM VALUES (NULL), (5), (20) AS tab(col);
-- SELECT first_value(col, true) FROM VALUES (NULL), (5), (20) AS tab(col);
-- SELECT flatten(array(array(1, 2), array(3, 4)));
-- SELECT floor(-0.1);
-- SELECT floor(5);
-- SELECT forall(array(1, 2, 3), x -> x % 2 == 0);
-- SELECT forall(array(2, 4, 8), x -> x % 2 == 0);
-- SELECT forall(array(1, null, 3), x -> x % 2 == 0);
-- SELECT forall(array(2, null, 8), x -> x % 2 == 0);
-- SELECT format_number(12332.123456, 4);
-- SELECT format_number(12332.123456, '##################.###');
-- SELECT format_string("Hello World %d %s", 100, "days");
-- SELECT from_csv('1, 0.8', 'a INT, b DOUBLE');
-- SELECT from_csv('26/08/2015', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));
-- SELECT from_json('{"a":1, "b":0.8}', 'a INT, b DOUBLE');
-- SELECT from_json('{"time":"26/08/2015"}', 'time Timestamp', map('timestampFormat', 'dd/MM/yyyy'));
-- SELECT from_unixtime(0, 'yyyy-MM-dd HH:mm:ss');
-- SELECT from_unixtime(0);
-- SELECT from_utc_timestamp('2016-08-31', 'Asia/Seoul');
-- SELECT get_json_object('{"a":"b"}', '$.a');
-- SELECT greatest(10, 9, 2, 4, 3);
-- SELECT name, grouping(name), sum(age) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY cube(name);
-- SELECT name, grouping_id(), sum(age), avg(height) FROM VALUES (2, 'Alice', 165), (5, 'Bob', 180) people(age, name, height) GROUP BY cube(name, height);
-- SELECT hash('Spark', array(123), 2);
-- SELECT hex(17);
-- SELECT hex('Spark SQL');
-- SELECT hour('2009-07-30 12:58:59');
-- SELECT hypot(3, 4);
-- SELECT if(1 < 2, 'a', 'b');
-- SELECT ifnull(NULL, array('2'));
-- SELECT 1 in(1, 2, 3);
-- SELECT 1 in(2, 3, 4);
-- SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 1), named_struct('a', 1, 'b', 3));
-- SELECT named_struct('a', 1, 'b', 2) in(named_struct('a', 1, 'b', 2), named_struct('a', 1, 'b', 3));
-- SELECT initcap('sPark sql');
-- SELECT inline(array(struct(1, 'a'), struct(2, 'b')));
-- SELECT inline_outer(array(struct(1, 'a'), struct(2, 'b')));
-- SELECT input_file_block_length();
-- SELECT input_file_block_start();
-- SELECT input_file_name();
-- SELECT instr('SparkSQL', 'SQL');
-- SELECT isnan(cast('NaN' as double));
-- SELECT isnotnull(1);
-- SELECT isnull(1);
-- SELECT java_method('java.util.UUID', 'randomUUID');
-- SELECT java_method('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');
-- SELECT json_array_length('[1,2,3,4]');
-- SELECT json_array_length('[1,2,3,{"f1":1,"f2":[5,6]},4]');
-- SELECT json_array_length('[1,2');
-- SELECT json_object_keys('{}');
-- SELECT json_object_keys('{"key": "value"}');
-- SELECT json_object_keys('{"f1":"abc","f2":{"f3":"a", "f4":"b"}}');
-- SELECT json_tuple('{"a":1, "b":2}', 'a', 'b');
-- SELECT kurtosis(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);
-- SELECT kurtosis(col) FROM VALUES (1), (10), (100), (10), (1) as tab(col);
-- SELECT a, b, lag(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT last(col) FROM VALUES (10), (5), (20) AS tab(col);
-- SELECT last(col) FROM VALUES (10), (5), (NULL) AS tab(col);
-- SELECT last(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);
-- SELECT last_day('2009-01-12');
-- SELECT last_value(col) FROM VALUES (10), (5), (20) AS tab(col);
-- SELECT last_value(col) FROM VALUES (10), (5), (NULL) AS tab(col);
-- SELECT last_value(col, true) FROM VALUES (10), (5), (NULL) AS tab(col);
-- SELECT lcase('SparkSql');
-- SELECT a, b, lead(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT least(10, 9, 2, 4, 3);
-- SELECT left('Spark SQL', 3);
-- SELECT length('Spark SQL ');
-- SELECT CHAR_LENGTH('Spark SQL ');
-- SELECT CHARACTER_LENGTH('Spark SQL ');
-- SELECT levenshtein('kitten', 'sitting');
-- SELECT like('Spark', '_park');
-- SELECT '%SystemDrive%\Users\John' like '\%SystemDrive\%\\Users%';
-- SELECT '%SystemDrive%\\Users\\John' like '\%SystemDrive\%\\\\Users%';
-- SELECT '%SystemDrive%/Users/John' like '/%SystemDrive/%//Users%' ESCAPE '/';
-- SELECT ln(1);
-- SELECT locate('bar', 'foobarbar');
-- SELECT locate('bar', 'foobarbar', 5);
-- SELECT POSITION('bar' IN 'foobarbar');
-- SELECT log(10, 100);
-- SELECT log10(10);
-- SELECT log1p(0);
-- SELECT log2(2);
-- SELECT lower('SparkSql');
-- SELECT lpad('hi', 5, '??');
-- SELECT lpad('hi', 1, '??');
-- SELECT lpad('hi', 5);
-- SELECT ltrim('    SparkSQL   ');
-- SELECT make_date(2013, 7, 15);
-- SELECT make_date(2019, 13, 1);
-- SELECT make_date(2019, 7, NULL);
-- SELECT make_date(2019, 2, 30);
-- SELECT make_interval(100, 11, 1, 1, 12, 30, 01.001001);
-- SELECT make_interval(100, null, 3);
-- SELECT make_interval(0, 1, 0, 1, 0, 0, 100.000001);
-- SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887);
-- SELECT make_timestamp(2014, 12, 28, 6, 30, 45.887, 'CET');
-- SELECT make_timestamp(2019, 6, 30, 23, 59, 60);
-- SELECT make_timestamp(2019, 13, 1, 10, 11, 12, 'PST');
-- SELECT make_timestamp(null, 7, 22, 15, 30, 0);
-- SELECT map(1.0, '2', 3.0, '4');
-- SELECT map_concat(map(1, 'a', 2, 'b'), map(3, 'c'));
-- SELECT map_entries(map(1, 'a', 2, 'b'));
-- SELECT map_filter(map(1, 0, 2, 2, 3, -1), (k, v) -> k > v);
-- SELECT map_from_arrays(array(1.0, 3.0), array('2', '4'));
-- SELECT map_from_entries(array(struct(1, 'a'), struct(2, 'b')));
-- SELECT map_keys(map(1, 'a', 2, 'b'));
-- SELECT map_values(map(1, 'a', 2, 'b'));
-- SELECT map_zip_with(map(1, 'a', 2, 'b'), map(1, 'x', 2, 'y'), (k, v1, v2) -> concat(v1, v2));
-- SELECT max(col) FROM VALUES (10), (50), (20) AS tab(col);
-- SELECT max_by(x, y) FROM VALUES (('a', 10)), (('b', 50)), (('c', 20)) AS tab(x, y);
-- SELECT md5('Spark');
-- SELECT mean(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT mean(col) FROM VALUES (1), (2), (NULL) AS tab(col);
-- SELECT min(col) FROM VALUES (10), (-1), (20) AS tab(col);
-- SELECT min_by(x, y) FROM VALUES (('a', 10)), (('b', 50)), (('c', 20)) AS tab(x, y);
-- SELECT minute('2009-07-30 12:58:59');
-- SELECT 2 % 1.8;
-- SELECT MOD(2, 1.8);
-- SELECT monotonically_increasing_id();
-- SELECT month('2016-07-30');
-- SELECT months_between('1997-02-28 10:30:00', '1996-10-30');
-- SELECT months_between('1997-02-28 10:30:00', '1996-10-30', false);
-- SELECT named_struct("a", 1, "b", 2, "c", 3);
-- SELECT nanvl(cast('NaN' as double), 123);
-- SELECT negative(1);
-- SELECT next_day('2015-01-14', 'TU');
-- SELECT not true;
-- SELECT not false;
-- SELECT not NULL;
-- SELECT now();
-- SELECT a, b, nth_value(b, 2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT a, b, ntile(2) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT nullif(2, 2);
-- SELECT nvl(NULL, array('2'));
-- SELECT nvl2(NULL, 2, 1);
-- SELECT octet_length('Spark SQL');
-- SELECT true or false;
-- SELECT false or false;
-- SELECT true or NULL;
-- SELECT false or NULL;
-- SELECT overlay('Spark SQL' PLACING '_' FROM 6);
-- SELECT overlay('Spark SQL' PLACING 'CORE' FROM 7);
-- SELECT overlay('Spark SQL' PLACING 'ANSI ' FROM 7 FOR 0);
-- SELECT overlay('Spark SQL' PLACING 'tructured' FROM 2 FOR 4);
-- SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('_', 'utf-8') FROM 6);
-- SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('CORE', 'utf-8') FROM 7);
-- SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('ANSI ', 'utf-8') FROM 7 FOR 0);
-- SELECT overlay(encode('Spark SQL', 'utf-8') PLACING encode('tructured', 'utf-8') FROM 2 FOR 4);
-- SELECT parse_url('http://spark.apache.org/path?query=1', 'HOST');
-- SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY');
-- SELECT parse_url('http://spark.apache.org/path?query=1', 'QUERY', 'query');
-- SELECT a, b, percent_rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT percentile(col, 0.3) FROM VALUES (0), (10) AS tab(col);
-- SELECT percentile(col, array(0.25, 0.75)) FROM VALUES (0), (10) AS tab(col);
-- SELECT percentile_approx(col, array(0.5, 0.4, 0.1), 100) FROM VALUES (0), (1), (2), (10) AS tab(col);
-- SELECT percentile_approx(col, 0.5, 100) FROM VALUES (0), (6), (7), (9), (10) AS tab(col);
-- SELECT pi();
-- SELECT pmod(10, 3);
-- SELECT pmod(-10, 3);
-- SELECT posexplode(array(10,20));
-- SELECT posexplode_outer(array(10,20));
-- SELECT position('bar', 'foobarbar');
-- SELECT position('bar', 'foobarbar', 5);
-- SELECT POSITION('bar' IN 'foobarbar');
-- SELECT positive(1);
-- SELECT pow(2, 3);
-- SELECT power(2, 3);
-- SELECT printf("Hello World %d %s", 100, "days");
-- SELECT quarter('2016-08-31');
-- SELECT radians(180);
-- SELECT raise_error('custom error message');
-- SELECT rand();
-- SELECT rand(0);
-- SELECT rand(null);
-- SELECT randn();
-- SELECT randn(0);
-- SELECT randn(null);
-- SELECT random();
-- SELECT random(0);
-- SELECT random(null);
-- SELECT a, b, rank(b) OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT reflect('java.util.UUID', 'randomUUID');
-- SELECT reflect('java.util.UUID', 'fromString', 'a5cf6c42-0c85-418f-af6c-3e4e5b1328f2');
-- SELECT regexp_extract('100-200', '(\\d+)-(\\d+)', 1);
-- SELECT regexp_extract_all('100-200, 300-400', '(\\d+)-(\\d+)', 1);
-- SELECT regexp_replace('100-200', '(\\d+)', 'num');
-- SELECT repeat('123', 2);
-- SELECT replace('ABCabc', 'abc', 'DEF');
-- SELECT reverse('Spark SQL');
-- SELECT reverse(array(2, 1, 4, 3));
-- SELECT right('Spark SQL', 3);
-- SELECT rint(12.3456);
-- SELECT '%SystemDrive%\Users\John' rlike '%SystemDrive%\\Users.*';
-- SELECT '%SystemDrive%\\Users\\John' rlike '%SystemDrive%\\\\Users.*';
-- SELECT name, age, count(*) FROM VALUES (2, 'Alice'), (5, 'Bob') people(age, name) GROUP BY rollup(name, age);
-- SELECT round(2.5, 0);
-- SELECT a, b, row_number() OVER (PARTITION BY a ORDER BY b) FROM VALUES ('A1', 2), ('A1', 1), ('A2', 3), ('A1', 1) tab(a, b);
-- SELECT rpad('hi', 5, '??');
-- SELECT rpad('hi', 1, '??');
-- SELECT rpad('hi', 5);
-- SELECT rtrim('    SparkSQL   ');
-- SELECT schema_of_csv('1,abc');
-- SELECT schema_of_json('[{"col":0}]');
-- SELECT schema_of_json('[{"col":01}]', map('allowNumericLeadingZeros', 'true'));
-- SELECT second('2009-07-30 12:58:59');
-- SELECT sentences('Hi there! Good morning.');
-- SELECT sequence(1, 5);
-- SELECT sequence(5, 1);
-- SELECT sequence(to_date('2018-01-01'), to_date('2018-03-01'), interval 1 month);
-- SELECT sha('Spark');
-- SELECT sha1('Spark');
-- SELECT sha2('Spark', 256);
-- SELECT shiftleft(2, 1);
-- SELECT shiftright(4, 1);
-- SELECT shiftrightunsigned(4, 1);
-- SELECT shuffle(array(1, 20, 3, 5));
-- SELECT shuffle(array(1, 20, null, 3));
-- SELECT sign(40);
-- SELECT signum(40);
-- SELECT sin(0);
-- SELECT sinh(0);
-- SELECT size(array('b', 'd', 'c', 'a'));
-- SELECT size(map('a', 1, 'b', 2));
-- SELECT size(NULL);
-- SELECT skewness(col) FROM VALUES (-10), (-20), (100), (1000) AS tab(col);
-- SELECT skewness(col) FROM VALUES (-1000), (-100), (10), (20) AS tab(col);
-- SELECT slice(array(1, 2, 3, 4), 2, 2);
-- SELECT slice(array(1, 2, 3, 4), -2, 2);
-- SELECT some(col) FROM VALUES (true), (false), (false) AS tab(col);
-- SELECT some(col) FROM VALUES (NULL), (true), (false) AS tab(col);
-- SELECT some(col) FROM VALUES (false), (false), (NULL) AS tab(col);
-- SELECT sort_array(array('b', 'd', null, 'c', 'a'), true);
-- SELECT soundex('Miller');
-- SELECT concat(space(2), '1');
-- SELECT spark_partition_id();
-- SELECT split('oneAtwoBthreeC', '[ABC]');
-- SELECT split('oneAtwoBthreeC', '[ABC]', -1);
-- SELECT split('oneAtwoBthreeC', '[ABC]', 2);
-- SELECT sqrt(4);
-- SELECT stack(2, 1, 2, 3);
-- SELECT std(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT stddev(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT stddev_pop(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT stddev_samp(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT str_to_map('a:1,b:2,c:3', ',', ':');
-- SELECT str_to_map('a');
-- SELECT struct(1, 2, 3);
-- SELECT substr('Spark SQL', 5);
-- SELECT substr('Spark SQL', -3);
-- SELECT substr('Spark SQL', 5, 1);
-- SELECT substr('Spark SQL' FROM 5);
-- SELECT substr('Spark SQL' FROM -3);
-- SELECT substr('Spark SQL' FROM 5 FOR 1);
-- SELECT substring('Spark SQL', 5);
-- SELECT substring('Spark SQL', -3);
-- SELECT substring('Spark SQL', 5, 1);
-- SELECT substring('Spark SQL' FROM 5);
-- SELECT substring('Spark SQL' FROM -3);
-- SELECT substring('Spark SQL' FROM 5 FOR 1);
-- SELECT substring_index('www.apache.org', '.', 2);
-- SELECT sum(col) FROM VALUES (5), (10), (15) AS tab(col);
-- SELECT sum(col) FROM VALUES (NULL), (10), (15) AS tab(col);
-- SELECT sum(col) FROM VALUES (NULL), (NULL) AS tab(col);
-- SELECT tan(0);
-- SELECT tanh(0);
-- SELECT timestamp_micros(1230219000123123);
-- SELECT timestamp_millis(1230219000123);
-- SELECT timestamp_seconds(1230219000);
-- SELECT timestamp_seconds(1230219000.123);
-- SELECT to_csv(named_struct('a', 1, 'b', 2));
-- SELECT to_csv(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));
-- SELECT to_date('2009-07-30 04:17:52');
-- SELECT to_date('2016-12-31', 'yyyy-MM-dd');
-- SELECT to_json(named_struct('a', 1, 'b', 2));
-- SELECT to_json(named_struct('time', to_timestamp('2015-08-26', 'yyyy-MM-dd')), map('timestampFormat', 'dd/MM/yyyy'));
-- SELECT to_json(array(named_struct('a', 1, 'b', 2)));
-- SELECT to_json(map('a', named_struct('b', 1)));
-- SELECT to_json(map(named_struct('a', 1),named_struct('b', 2)));
-- SELECT to_json(map('a', 1));
-- SELECT to_json(array((map('a', 1))));
-- SELECT to_timestamp('2016-12-31 00:12:00');
-- SELECT to_timestamp('2016-12-31', 'yyyy-MM-dd');
-- SELECT to_unix_timestamp('2016-04-08', 'yyyy-MM-dd');
-- SELECT to_utc_timestamp('2016-08-31', 'Asia/Seoul');
-- SELECT transform(array(1, 2, 3), x -> x + 1);
-- SELECT transform(array(1, 2, 3), (x, i) -> x + i);
-- SELECT transform_keys(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + 1);
-- SELECT transform_keys(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + v);
-- SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> v + 1);
-- SELECT transform_values(map_from_arrays(array(1, 2, 3), array(1, 2, 3)), (k, v) -> k + v);
-- SELECT translate('AaBbCc', 'abc', '123');
-- SELECT trim('    SparkSQL   ');
-- SELECT trim(BOTH FROM '    SparkSQL   ');
-- SELECT trim(LEADING FROM '    SparkSQL   ');
-- SELECT trim(TRAILING FROM '    SparkSQL   ');
-- SELECT trim('SL' FROM 'SSparkSQLS');
-- SELECT trim(BOTH 'SL' FROM 'SSparkSQLS');
-- SELECT trim(LEADING 'SL' FROM 'SSparkSQLS');
-- SELECT trim(TRAILING 'SL' FROM 'SSparkSQLS');
-- SELECT trunc('2019-08-04', 'week');
-- SELECT trunc('2019-08-04', 'quarter');
-- SELECT trunc('2009-02-12', 'MM');
-- SELECT trunc('2015-10-27', 'YEAR');
-- SELECT typeof(1);
-- SELECT typeof(array(1));
-- SELECT ucase('SparkSql');
-- SELECT unbase64('U3BhcmsgU1FM');
-- SELECT decode(unhex('537061726B2053514C'), 'UTF-8');
-- SELECT unix_date(DATE("1970-01-02"));
-- SELECT unix_micros(TIMESTAMP('1970-01-01 00:00:01Z'));
-- SELECT unix_millis(TIMESTAMP('1970-01-01 00:00:01Z'));
-- SELECT unix_seconds(TIMESTAMP('1970-01-01 00:00:01Z'));
-- SELECT unix_timestamp();
-- SELECT unix_timestamp('2016-04-08', 'yyyy-MM-dd');
-- SELECT upper('SparkSql');
-- SELECT uuid();
-- SELECT var_pop(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT var_samp(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT variance(col) FROM VALUES (1), (2), (3) AS tab(col);
-- SELECT version();
-- SELECT weekday('2009-07-30');
-- SELECT weekofyear('2008-02-20');
-- SELECT CASE WHEN 1 > 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;
-- SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 > 0 THEN 2.0 ELSE 1.2 END;
-- SELECT CASE WHEN 1 < 0 THEN 1 WHEN 2 < 0 THEN 2.0 END;
-- SELECT width_bucket(5.3, 0.2, 10.6, 5);
-- SELECT width_bucket(-2.1, 1.3, 3.4, 3);
-- SELECT width_bucket(8.1, 0.0, 5.7, 4);
-- SELECT width_bucket(-0.9, 5.2, 0.5, 2);
-- SELECT xpath('<a><b>b1</b><b>b2</b><b>b3</b><c>c1</c><c>c2</c></a>','a/b/text()');
-- SELECT xpath_boolean('<a><b>1</b></a>','a/b');
-- SELECT xpath_double('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
-- SELECT xpath_float('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
-- SELECT xpath_int('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
-- SELECT xpath_long('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
-- SELECT xpath_number('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
-- SELECT xpath_short('<a><b>1</b><b>2</b></a>', 'sum(a/b)');
-- SELECT xpath_string('<a><b>b</b><c>cc</c></a>','a/c');
-- SELECT xxhash64('Spark', array(123), 2);
-- SELECT year('2016-07-30');
-- SELECT zip_with(array(1, 2, 3), array('a', 'b', 'c'), (x, y) -> (y, x));
-- SELECT zip_with(array(1, 2), array(3, 4), (x, y) -> x + y);
-- SELECT zip_with(array('a', 'b', 'c'), array('d', 'e', 'f'), (x, y) -> concat(x, y));
-- SELECT 3 | 5;
-- SELECT 'Spark' || 'SQL';
-- SELECT array(1, 2, 3) || array(4, 5) || array(6);
-- SELECT ~ 0;

Reference: https://spark.apache.org/docs/latest/api/sql/index.html
